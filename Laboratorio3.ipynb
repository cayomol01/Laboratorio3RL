{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ¿Qué es Programación Dinámica y cómo se relaciona con RL?\n",
    "\n",
    "    Programación Dinámica (PD) es una técnica de optimización que resuelve problemas complejos dividiéndolos\n",
    "    en subproblemas más simples y almacenando los resultados de estos subproblemas para evitar cálculos redundantes. \n",
    "    Fue desarrollada por Richard Bellman en la\n",
    "    década de 1950 y se utiliza para resolver problemas de optimización que pueden descomponerse en etapas secuenciales\n",
    "    \n",
    "    En el contexto de Reinforcement Learning (RL), la programación dinámica se aplica en métodos como Iteración de\n",
    "    valor e Iteración de Póliza. Estos métodos buscan encontrar la política óptima para un agente en un entorno de\n",
    "    MDP (Markov Decision Process) utilizando principios de PD. RL puede ser visto como una extensión de PD donde \n",
    "    el entorno puede no ser completamente conocido de antemano, requiriendo que el agente explore y aprenda a partir \n",
    "    de la interacción con el entorno.\n",
    "\n",
    "\n",
    "### 2. Explique en sus propias palabras el algoritmo de Iteración de Póliza.\n",
    "\n",
    "    Iteración de Póliza es un algoritmo de PD utilizado en RL para encontrar la política óptima en un MDP. El proceso \n",
    "    alterna entre dos fases: evaluación de la póliza y mejora de la póliza.\n",
    "\n",
    "    Evaluación de la Póliza: Dada una política (estrategia) inicial, se calcula el valor esperado de cada estado \n",
    "    siguiendo esa política. Esto implica resolver un sistema de ecuaciones para obtener los valores de estado.\n",
    "    Mejora de la Póliza: Utilizando los valores de estado calculados, se actualiza la política para elegir las \n",
    "    acciones que maximicen el valor esperado en cada estado.\n",
    "    El algoritmo repite estas fases hasta que la política deja de cambiar significativamente, lo que indica que\n",
    "    se ha encontrado la política óptima.\n",
    "    \n",
    "### 3. Explique en sus propias palabras el algoritmo de Iteración de Valor\n",
    "\n",
    "    Iteración de Valor es otro algoritmo de PD en RL que busca la política óptima iterando sobre los valores de \n",
    "    los estados directamente.\n",
    "    Actualización de Valor: Se actualiza el valor de cada estado iterativamente utilizando la ecuación de Bellman. \n",
    "    Esta ecuación expresa el valor de un estado como el valor esperado de la recompensa inmediata más el valor \n",
    "    esperado del próximo estado, considerando todas las posibles acciones y transiciones.\n",
    "    Derivación de la Póliza: Una vez que los valores de estado se estabilizan (convergen), se deriva la política\n",
    "    óptima eligiendo en cada estado la acción que maximice el valor esperado del siguiente estado.\n",
    "    El proceso de iteración de valor continúa hasta que los valores de estado convergen a los valores óptimos, \n",
    "    momento en el cual la política óptima puede ser extraída fácilmente.\n",
    " \n",
    "### 4. En el laboratorio pasado,vimos que elvalor de los premios obtenidos se mantienen constantes, ¿porqué?\n",
    "\n",
    "    El valor de los premios obtenidos se mantiene constante debido a la naturaleza de la función de recompensa en el \n",
    "    problema específico tratado. En muchos entornos de RL, las recompensas están definidas por una función fija que \n",
    "    no varía con el tiempo ni con las acciones del agente. Esto significa que, independientemente de las decisiones \n",
    "    que tome el agente, las recompensas asociadas a estados y transiciones particulares permanecen iguales.\n",
    "\n",
    "    Esta constancia en las recompensas facilita el análisis y la solución de MDPs, ya que permite la aplicación directa\n",
    "    de algoritmos de PD como la Iteración de Póliza e Iteración de Valor sin necesidad de ajustar las recompensas en \n",
    "    cada iteración.\n",
    "\n",
    "\n",
    "\n",
    "### Referencias\n",
    "    Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP():\n",
    "    def __init__(self):\n",
    "        self.states = self.getStates()\n",
    "        self.actions = self.getActions()\n",
    "        self.map = self.getMap()\n",
    "        self.transitions = self.getTransitions()\n",
    "        self.rewards = self.getRewards()\n",
    "        self.policy = self.getPolicy()\n",
    "        self.V = {s: 0 for s in self.states}  # Inicializamos la función de valor en 0\n",
    "        self.gamma = 0.9  # Factor de descuento\n",
    "\n",
    "    def simulatePolicy(self, steps, initialState=0):\n",
    "        rewardAcc = 0\n",
    "        estado = initialState\n",
    "        for _ in range(steps):\n",
    "            action = self.policy[estado]\n",
    "            sPrime = self.transitions[estado][action]\n",
    "            reward = self.rewards[estado][action][sPrime]\n",
    "            \n",
    "            rewardAcc += reward\n",
    "            estado = sPrime\n",
    "            \n",
    "            if estado == 5:  # Si el robot alcanza la meta (G), terminamos la simulación\n",
    "                break\n",
    "        return rewardAcc\n",
    "    \n",
    "    def getStates(self):\n",
    "        states = [i for i in range(9)]\n",
    "        return states\n",
    "\n",
    "    def getMapSymbol(self, pos):\n",
    "        return self.map[pos]\n",
    "\n",
    "    def getRewards(self):\n",
    "        rewards = {}\n",
    "        for i in self.states:\n",
    "            rewards[i] = {}\n",
    "            for j in self.actions:\n",
    "                sPrime = self.transitions[i][j]\n",
    "                if self.getMapSymbol(sPrime) == 'G':\n",
    "                    rewards[i][j] = {sPrime: 1}\n",
    "                elif self.getMapSymbol(sPrime) == 'X':\n",
    "                    rewards[i][j] = {sPrime: -1}\n",
    "                else:\n",
    "                    rewards[i][j] = {sPrime: 0}\n",
    "        return rewards\n",
    "\n",
    "    def getPolicy(self):\n",
    "        policy = {}\n",
    "        for i in self.states:\n",
    "            policy[i] = random.choice(self.actions)\n",
    "        return policy\n",
    "\n",
    "    def checkPosition(self, state, action):\n",
    "        if action == 'arriba':\n",
    "            if (state - 3) < 0:\n",
    "                return state\n",
    "            else:\n",
    "                return state - 3\n",
    "        elif action == 'abajo':\n",
    "            if (state + 3) > len(self.states) - 1:\n",
    "                return state\n",
    "            else:\n",
    "                return state + 3\n",
    "        elif action == 'izquierda':\n",
    "            if (state % 3) == 0:\n",
    "                return state\n",
    "            else:\n",
    "                return state - 1\n",
    "        elif action == 'derecha':\n",
    "            if (state % 3) == 2:\n",
    "                return state\n",
    "            else:\n",
    "                return state + 1\n",
    "\n",
    "    def getTransitions(self):\n",
    "        p = {}\n",
    "        for i in self.states:\n",
    "            p[i] = {}\n",
    "            for j in self.actions:\n",
    "                p[i][j] = self.checkPosition(i, j)\n",
    "        return p\n",
    "\n",
    "    def getMap(self):\n",
    "        map = ['S', ' ', 'X',\n",
    "               ' ', 'X', ' ',\n",
    "               ' ', ' ', 'G']\n",
    "        return map\n",
    "\n",
    "    def getActions(self):\n",
    "        actions = ['arriba', 'abajo', 'izquierda', 'derecha']\n",
    "        return actions\n",
    "\n",
    "    def valueIteration(self, threshold=0.001):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                v = self.V[s]\n",
    "                max_value = float('-inf')\n",
    "                for a in self.actions:\n",
    "                    s_prime = self.transitions[s][a]\n",
    "                    reward = self.rewards[s][a][s_prime]\n",
    "                    value = reward + self.gamma * self.V[s_prime]\n",
    "                    if value > max_value:\n",
    "                        max_value = value\n",
    "                self.V[s] = max_value\n",
    "                delta = max(delta, abs(v - self.V[s]))\n",
    "            if delta < threshold:\n",
    "                break\n",
    "        self.extractPolicy()\n",
    "\n",
    "    def extractPolicy(self):\n",
    "        policy = {}\n",
    "        for s in self.states:\n",
    "            max_value = float('-inf')\n",
    "            best_action = None\n",
    "            for a in self.actions:\n",
    "                s_prime = self.transitions[s][a]\n",
    "                reward = self.rewards[s][a][s_prime]\n",
    "                value = reward + self.gamma * self.V[s_prime]\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_action = a\n",
    "            policy[s] = best_action\n",
    "        self.policy = policy\n",
    "\n",
    "    def policyIteration(self):\n",
    "        stable = False\n",
    "        while not stable:\n",
    "            stable = True\n",
    "            self.evaluatePolicy()\n",
    "            for s in self.states:\n",
    "                current_action = self.policy[s]\n",
    "                max_value = float('-inf')\n",
    "                best_action = None\n",
    "                for a in self.actions:\n",
    "                    s_prime = self.transitions[s][a]\n",
    "                    reward = self.rewards[s][a][s_prime]\n",
    "                    value = reward + self.gamma * self.V[s_prime]\n",
    "                    if value > max_value:\n",
    "                        max_value = value\n",
    "                        best_action = a\n",
    "                if current_action != best_action:\n",
    "                    self.policy[s] = best_action\n",
    "                    stable = False\n",
    "\n",
    "    def evaluatePolicy(self, threshold=0.001):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in self.states:\n",
    "                v = self.V[s]\n",
    "                a = self.policy[s]\n",
    "                s_prime = self.transitions[s][a]\n",
    "                reward = self.rewards[s][a][s_prime]\n",
    "                self.V[s] = reward + self.gamma * self.V[s_prime]\n",
    "                delta = max(delta, abs(v - self.V[s]))\n",
    "            if delta < threshold:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'arriba', 1: 'arriba', 2: 'arriba', 3: 'izquierda', 4: 'izquierda', 5: 'abajo', 6: 'izquierda', 7: 'abajo', 8: 'izquierda'}\n",
      "\n",
      "Función de valor óptimo (Iteración de valor): {0: 7.281404955442832, 1: 7.091404955442831, 2: 8.991404955442832, 3: 8.091404955442831, 4: 8.991404955442832, 5: 9.991404955442832, 6: 8.991404955442832, 7: 9.991404955442832, 8: 9.991404955442832}\n",
      "\n",
      "Política óptima (Iteración de valor): {0: 'abajo', 1: 'abajo', 2: 'abajo', 3: 'abajo', 4: 'abajo', 5: 'abajo', 6: 'derecha', 7: 'derecha', 8: 'abajo'}\n",
      "\n",
      "Función de valor óptimo (Iteración de políticas): {0: 7.282264459898548, 1: 7.09226445989855, 2: 8.992264459898548, 3: 8.09226445989855, 4: 8.992264459898548, 5: 9.992264459898548, 6: 8.992264459898548, 7: 9.992264459898548, 8: 9.992264459898548}\n",
      "\n",
      "Política óptima (Iteración de políticas): {0: 'abajo', 1: 'abajo', 2: 'abajo', 3: 'abajo', 4: 'abajo', 5: 'abajo', 6: 'derecha', 7: 'derecha', 8: 'abajo'}\n"
     ]
    }
   ],
   "source": [
    "mdp = MDP()\n",
    "\n",
    "print(mdp.policy)\n",
    "# Iteración de valor\n",
    "mdp.valueIteration()\n",
    "print()\n",
    "print(\"Función de valor óptimo (Iteración de valor):\", mdp.V)\n",
    "print()\n",
    "print(\"Política óptima (Iteración de valor):\", mdp.policy)\n",
    "\n",
    "# Iteración de políticas\n",
    "mdp.policyIteration()\n",
    "print()\n",
    "print(\"Función de valor óptimo (Iteración de políticas):\", mdp.V)\n",
    "print()\n",
    "print(\"Política óptima (Iteración de políticas):\", mdp.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
